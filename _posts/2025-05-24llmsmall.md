JigsawStack Logo
Beta

What Even Is a Small Language Model Now?
15 May 2025


Yoeven D Khemlani

Share this article




What Even Is a Small Language Model Now?
If you asked someone in 2018 what a "small model" was, they'd probably say something with a few million parameters that ran on a Raspberry Pi or your phone. Fast-forward to today, and we're calling 30B parameter models "small"â€”because they only need one GPU to run.

So yeah, the definition of "small" has changed.

Small Used to Mean... Actually Small
Back in the early days of machine learning, a "small model" might've been a decision tree or a basic neural net that could run on a laptop CPU. Think scikit-learn, not LLMs.

Then came transformers and large language models (LLMs). As these got bigger and better, anything not requiring a cluster of A100s suddenly started to feel... small by comparison.

Today, small is more about how deployable the model is, not just its size on paper.

https://jigsawstack.com/blog/what-even-is-a-small-language-model-now--ai